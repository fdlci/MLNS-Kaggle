{"cells":[{"metadata":{"id":"IO285KLTtjqw","trusted":true},"cell_type":"code","source":["%matplotlib inline\n","import os\n","import networkx as nx\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.sparse import *\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score, roc_curve, auc\n","import pandas as pd\n","import csv\n","from sklearn import svm\n","from sklearn import ensemble\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import linear_kernel\n","from sklearn import preprocessing\n","from sklearn.metrics.pairwise import cosine_similarity\n","import nltk\n","import random\n","from sklearn.metrics import accuracy_score, f1_score\n","from tqdm.notebook import tqdm"],"execution_count":19,"outputs":[]},{"metadata":{"id":"09TZq7WAtjqy","outputId":"073edea1-a7e3-4207-d535-7d76cd4544fa","trusted":true},"cell_type":"code","source":["nltk.download('punkt') # for tokenization\n","nltk.download('stopwords')\n","stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n","stemmer = nltk.stem.PorterStemmer()"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\remys\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\remys\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"metadata":{"id":"7CrhWJTOtjqz","trusted":true},"cell_type":"code","source":["# Info of the graph\n","def compute_network_characteristics(graph):\n","    prop = {}\n","    prop['N'] =  graph.number_of_nodes() # number of nodes\n","    prop['M'] = graph.number_of_edges() # number of edges\n","    degrees = [degree for node, degree in graph.degree()] # degree list\n","    prop['min_degree'] =  np.min(degrees) # minimum degree\n","    prop['max_degree'] =  np.max(degrees) # maximum degree\n","    prop['mean_degree'] = np.mean(degrees) # mean of node degrees\n","    prop['median_degree'] = np.median(degrees) # median of node degrees\n","    prop['density'] =  nx.density(graph) # density of the graph\n","    return prop"],"execution_count":3,"outputs":[]},{"metadata":{"id":"smMzjjgltjqz"},"cell_type":"markdown","source":["# Turning the training data into a graph"]},{"metadata":{"id":"_LNDYxKntjqz","trusted":true},"cell_type":"code","source":["def get_training_graph(csv_file, column_names=['source', 'target', 'connected']):\n","\n","    # Pandas dataframe\n","    df = pd.read_csv(csv_file, sep=' ', names=column_names)\n","    edges = df.loc[df['connected'] == 1]\n","\n","    # Number of edges\n","    print(f'There are {len(edges)} edges and {len(df) - len(edges)} non edges')\n","\n","    # networkx graph\n","    Graphtype = nx.DiGraph()\n","    G = nx.from_pandas_edgelist(edges, create_using=Graphtype)\n","    \n","    return G"],"execution_count":4,"outputs":[]},{"metadata":{"id":"P7uaxXrStjq0","outputId":"03aa46bb-0aed-44ba-895d-54001e6361e9","trusted":true},"cell_type":"code","source":["training_graph = get_training_graph('training_set.txt')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 335130 edges and 280382 non edges\n"]}]},{"metadata":{"id":"rm1We2Ejtjq0","outputId":"e488f416-4db1-4440-dedc-a5a72628d491","trusted":true},"cell_type":"code","source":["compute_network_characteristics(graph=training_graph)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'N': 27684,\n"," 'M': 335130,\n"," 'min_degree': 1,\n"," 'max_degree': 2346,\n"," 'mean_degree': 24.211096662332032,\n"," 'median_degree': 14.0,\n"," 'density': 0.0004372917794735403}"]},"metadata":{},"execution_count":6}]},{"metadata":{"id":"i40osmhatjq0"},"cell_type":"markdown","source":["## For now let's focus on a smaller graph"]},{"metadata":{"id":"k4MwiQsctjq0","trusted":true},"cell_type":"code","source":["with open(\"training_set.txt\", \"r\") as f:\n","    reader = csv.reader(f)\n","    training_set  = list(reader)\n","\n","training_set = [element[0].split(\" \") for element in training_set]"],"execution_count":7,"outputs":[]},{"metadata":{"id":"iHj23Uy6tjq1","outputId":"129284e9-8a61-432e-ce01-3acf51082e40","trusted":true},"cell_type":"code","source":["random.seed(10)\n","to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*1.)))\n","training_set_reduced = [training_set[i] for i in to_keep]"],"execution_count":8,"outputs":[]},{"metadata":{"id":"-NPXj8Rqtjq1","outputId":"e3506558-8182-4193-c7af-3c0d2c4c7902","trusted":true},"cell_type":"code","source":["# Pandas dataframe\n","df = pd.DataFrame(training_set_reduced, columns=['source', 'target', 'connected'])\n","edges = df.loc[df['connected'] == '1']\n","\n","# Number of edges\n","print(f'There are {len(edges)} edges and {len(df) - len(edges)} non edges')\n","\n","# networkx graph !!!!!!! The graph is directed\n","Graphtype = nx.DiGraph()\n","G = nx.from_pandas_edgelist(edges, create_using=Graphtype)\n","\n","# Take the largest weakly conected component\n","nodes = max(nx.strongly_connected_components(G), key=len) \n","G0 = G.subgraph(nodes)\n","\n","# Make that graph undirected\n","#G0 = G0.to_undirected()\n","nx.is_strongly_connected(G0)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 335130 edges and 280382 non edges\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"metadata":{"id":"91MyWPm2tjq1","outputId":"710120f3-19eb-446a-bfea-3cb3876028f5","trusted":true},"cell_type":"code","source":["compute_network_characteristics(G)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'N': 27684,\n"," 'M': 335130,\n"," 'min_degree': 1,\n"," 'max_degree': 2346,\n"," 'mean_degree': 24.211096662332032,\n"," 'median_degree': 14.0,\n"," 'density': 0.0004372917794735403}"]},"metadata":{},"execution_count":10}]},{"metadata":{"id":"JkPe4bJotjq1"},"cell_type":"markdown","source":["## Generate Samples"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def real_labels(graph, samples):\n","    labels = []\n","    for edge in tqdm(samples):\n","        if edge in graph.edges():\n","            labels.append(1)\n","        else:\n","            labels.append(0)\n","    return labels"],"execution_count":13,"outputs":[]},{"metadata":{"id":"kGRwLwbytjq2","trusted":true},"cell_type":"code","source":["def generate_samples(graph, train_set_ratio):\n","    \"\"\"\n","    Graph pre-processing step required to perform supervised link prediction\n","    Create training and test sets\n","    \"\"\"\n","        \n","    # --- Step 0: The graph must be connected ---\n","    if nx.is_strongly_connected(graph) is not True:\n","        raise ValueError(\"The graph contains more than one connected component!\")\n","       \n","    \n","    # --- Step 1: Generate positive edge samples for testing set ---\n","    residual_g = graph.copy()\n","    test_pos_samples = []\n","      \n","    # Store the shuffled list of current edges of the graph\n","    edges = list(residual_g.edges())\n","    np.random.shuffle(edges)\n","    \n","    # Define number of positive test samples desired\n","    test_set_size = int((1.0 - train_set_ratio) * graph.number_of_edges())\n","    train_set_size = graph.number_of_edges() - test_set_size\n","    num_of_pos_test_samples = 0\n","    \n","    # Remove random edges from the graph, leaving it connected\n","    # Fill in the blanks\n","    for edge in tqdm(edges):\n","        \n","        # Remove the edge\n","        residual_g.remove_edge(edge[0], edge[1])\n","        \n","        # Add the removed edge to the positive sample list if the network is still connected\n","        if nx.is_strongly_connected(residual_g):\n","            num_of_pos_test_samples += 1\n","            test_pos_samples.append(edge)\n","        # Otherwise, re-add the edge to the network\n","        else: \n","            residual_g.add_edge(edge[0], edge[1])\n","        \n","        # If we have collected enough number of edges for testing set, we can terminate the loop\n","        if num_of_pos_test_samples == test_set_size:\n","            break\n","    \n","    # Check if we have the desired number of positive samples for testing set \n","    if num_of_pos_test_samples != test_set_size:\n","        raise ValueError(\"Enough positive edge samples could not be found!\")\n","\n","        \n","    # --- Step 2: Generate positive edge samples for training set ---\n","    # The remaining edges are simply considered for positive samples of the training set\n","    train_pos_samples = list(residual_g.edges())\n","        \n","        \n","    # --- Step 3: Generate the negative samples for testing and training sets ---\n","    # Fill in the blanks\n","    non_edges = list(nx.non_edges(graph))\n","    random.seed(10)\n","    np.random.shuffle(non_edges)\n","    \n","    train_neg_samples = non_edges[:train_set_size] \n","    test_neg_samples = non_edges[train_set_size:train_set_size + test_set_size]\n","\n","    \n","    # --- Step 4: Combine sample lists and create corresponding labels ---\n","    # For training set\n","    train_samples = train_pos_samples + train_neg_samples\n","    train_labels = [1 for _ in train_pos_samples] + [0 for _ in train_neg_samples]\n","    # For testing set\n","    test_samples = test_pos_samples + test_neg_samples\n","    test_labels = [1 for _ in test_pos_samples] + [0 for _ in test_neg_samples]\n","    \n","    return residual_g, train_samples, train_labels, test_samples, test_labels\n"],"execution_count":14,"outputs":[]},{"metadata":{"id":"4dSSZK6Itjq2","outputId":"b4e16caf-0398-4905-faf4-cd6178a010af","trusted":true},"cell_type":"code","source":["residual_g, train_samples, train_labels, valid_samples, valid_labels = generate_samples(G0, train_set_ratio=0.8)"],"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/107612 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c29646f6c3f4c8aa648c2ba7ebcc1b0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["('9906191', '9903178')\n","('8091', '5087')\n","('9908115', '9908040')\n","('101085', '9912204')\n","('106231', '9191')\n","('207107', '111160')\n","('9802173', '9710178')\n","('9911160', '9906201')\n","('6112', '9803031')\n","('104043', '9910053')\n","('207117', '3071')\n","('9811042', '9806021')\n","('9804058', '9802150')\n","('11033', '10028')\n","('9705055', '9702187')\n","('108085', '106103')\n","('11073', '9908130')\n","('5067', '9905212')\n","('105090', '9707093')\n","('2084', '1215')\n","('108172', '105070')\n","('9909218', '9902111')\n","('9056', '9905012')\n","('8127', '9911116')\n","('9907086', '9711131')\n","('2205', '9910238')\n","('9807213', '9712166')\n","('9710030', '9706130')\n","('106014', '4148')\n","('9707217', '9702136')\n","('12180', '11008')\n","('10241', '9805086')\n","('9709113', '9706097')\n","('102063', '2211')\n","('111030', '104073')\n","('101192', '10151')\n","('3024', '9910263')\n","('9901144', '9811257')\n","('4085', '3145')\n","('4056', '9905012')\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-15-1abd319fa12f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresidual_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-14-a0b272eae1eb>\u001b[0m in \u001b[0;36mgenerate_samples\u001b[1;34m(graph, train_set_ratio)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Add the removed edge to the positive sample list if the network is still connected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_strongly_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresidual_g\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mnum_of_pos_test_samples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mtest_pos_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<decorator-gen-314>\u001b[0m in \u001b[0;36mis_strongly_connected\u001b[1;34m(G)\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\ressim\\lib\\site-packages\\networkx\\utils\\decorators.py\u001b[0m in \u001b[0;36m_not_implemented_for\u001b[1;34m(not_implement_for_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetworkXNotImplemented\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnot_implement_for_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_not_implemented_for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\ressim\\lib\\site-packages\\networkx\\algorithms\\components\\strongly_connected.py\u001b[0m in \u001b[0;36mis_strongly_connected\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m    337\u001b[0m             \"\"\"Connectivity is undefined for the null graph.\"\"\")\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrongly_connected_components\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\ressim\\lib\\site-packages\\networkx\\algorithms\\components\\strongly_connected.py\u001b[0m in \u001b[0;36mstrongly_connected_components\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m    106\u001b[0m                                 \u001b[0mlowlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlowlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                                 \u001b[0mlowlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlowlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreorder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m                     \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlowlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpreorder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"W3ui8XLPtjq3"},"cell_type":"markdown","source":["## Get the feature Vector"]},{"metadata":{"id":"-B2nHrakKsNc","outputId":"db9ec6e3-1488-4037-95cf-1940bfb0370a","trusted":true},"cell_type":"code","source":["print(\"degree centrality\")\n","deg_centrality = nx.degree_centrality(G0)\n","print(\"done!\")\n","\n","# katz_cent = nx.katz_centrality(G)\n","  \n","# print('betweeness_centrality')\n","# betweeness_centrality = nx.betweenness_centrality(G0)\n","# print('done!')"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["degree centrality\n","done!\n"]}]},{"metadata":{"id":"3cztGi7_tjq3","trusted":true},"cell_type":"code","source":["def feature_extractor(graph, samples, deg_centrality):\n","    \"\"\"\n","    Creates a feature vector for each edge of the graph contained in samples \n","    \"\"\"\n","    feature_vector = []\n","    number_nodes_out = 0\n","\n","    for edge in tqdm(samples):\n","        source_node, target_node = edge[0], edge[1]\n","\n","        # Degree Centrality\n","        if (source_node not in list(deg_centrality.keys())) or (target_node not in list(deg_centrality.keys())):\n","            feature_vector.append(np.array([0, 0, 0, 0, 0, 0]))\n","            number_nodes_out += 1\n","\n","        else:\n","\n","            source_degree_centrality = deg_centrality[source_node]\n","            target_degree_centrality = deg_centrality[target_node]\n","#             source_katz_cent = katz_cent[source_node]\n","#             target_katz_cent = katz_cent[target_node]\n","            \n","            # # Betweeness centrality measure \n","            #diff_bt = betweeness_centrality[target_node] - betweeness_centrality[source_node]\n","\n","            # Preferential Attachement \n","            pref_attach = list(nx.preferential_attachment(graph, [(source_node, target_node)]))[0][2]\n","\n","            # AdamicAdar\n","            aai = list(nx.adamic_adar_index(graph, [(source_node, target_node)]))[0][2]\n","\n","            # Jaccard\n","            jacard_coeff = list(nx.jaccard_coefficient(graph, [(source_node, target_node)]))[0][2]\n","            # Ressource allocation index\n","            res_all = list(nx.resource_allocation_index(graph, [(source_node, target_node)]))[0][2]\n","            \n","            # Create edge feature vector with all metric computed above\n","            feature_vector.append(np.array([source_degree_centrality, target_degree_centrality, pref_attach, aai, jacard_coeff, res_all]) ) \n","    print(number_nodes_out)\n","        \n","    return np.array(feature_vector)"],"execution_count":15,"outputs":[]},{"metadata":{"id":"R25kXICGtjq3","outputId":"743c7f58-a91c-4f78-bb23-d8bfde0ac673","trusted":true},"cell_type":"code","source":["train_features = feature_extractor(G0.to_undirected(), train_samples, deg_centrality)\n","valid_features = feature_extractor(G0.to_undirected(), valid_samples, deg_centrality)"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5291ad7a549b45e78d33cb66d0809db7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/43044 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32f6fb2bf287454e8fae706101050216"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"metadata":{"id":"5uqy9HOQtjq3","trusted":true},"cell_type":"code","source":["feat_train = pd.DataFrame(train_features, columns=['source_degree_centrality', 'target_degree_centrality', 'pref_attach', 'aai', 'jacard_coeff', 'res_all'])\n","feat_valid = pd.DataFrame(valid_features, columns=['source_degree_centrality', 'target_degree_centrality', 'pref_attach', 'aai', 'jacard_coeff', 'res_all'])"],"execution_count":17,"outputs":[]},{"metadata":{"id":"eD27sVzrtjq4","outputId":"15e54dc8-9efd-458a-a547-55ef5dcf414d","trusted":true},"cell_type":"code","source":["feat_train.head()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   source_degree_centrality  target_degree_centrality  pref_attach       aai  \\\n","0                  0.001245                  0.004012        261.0  0.000000   \n","1                  0.001245                  0.002767        180.0  0.248425   \n","2                  0.001245                  0.007748        504.0  0.648466   \n","3                  0.001245                  0.000830         54.0  0.000000   \n","4                  0.005949                  0.003735       1161.0  0.428700   \n","\n","   jacard_coeff   res_all  \n","0      0.000000  0.000000  \n","1      0.035714  0.017857  \n","2      0.031746  0.091667  \n","3      0.000000  0.000000  \n","4      0.029412  0.020333  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_degree_centrality</th>\n      <th>target_degree_centrality</th>\n      <th>pref_attach</th>\n      <th>aai</th>\n      <th>jacard_coeff</th>\n      <th>res_all</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001245</td>\n      <td>0.004012</td>\n      <td>261.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001245</td>\n      <td>0.002767</td>\n      <td>180.0</td>\n      <td>0.248425</td>\n      <td>0.035714</td>\n      <td>0.017857</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001245</td>\n      <td>0.007748</td>\n      <td>504.0</td>\n      <td>0.648466</td>\n      <td>0.031746</td>\n      <td>0.091667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001245</td>\n      <td>0.000830</td>\n      <td>54.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.005949</td>\n      <td>0.003735</td>\n      <td>1161.0</td>\n      <td>0.428700</td>\n      <td>0.029412</td>\n      <td>0.020333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":18}]},{"source":["## Prediction"],"cell_type":"markdown","metadata":{}},{"metadata":{"id":"8VTyZey5tjq5","trusted":true},"cell_type":"code","source":["# --- Build the model and train it ---\n","\n","#Scale the datas\n","scaler = preprocessing.MinMaxScaler()\n","scaled_train_features = scaler.fit_transform(train_features)\n","scaled_valid_features = scaler.transform(valid_features)\n","\n","#Predict\n","clf = LogisticRegression()\n","clf.fit(scaled_train_features, train_labels)\n","\n","train_preds = clf.predict_proba(scaled_train_features)[:, 1]\n","valid_preds = clf.predict_proba(scaled_valid_features)[:, 1]\n","labels_pred = clf.predict(scaled_valid_features)\n","\n","print(f'Accuracy: {accuracy_score(valid_labels, labels_pred)}')\n","print(f'F1 score: {f1_score(valid_labels, labels_pred)}')\n","\n","# --- Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from predictions ---\n","# Fill in the blanks\n","fpr, tpr, _ = roc_curve(valid_labels, valid_preds)\n","roc_auc = auc(fpr, tpr)\n","print(roc_auc)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9283523836074714\nF1 score: 0.9265924021708083\n0.9761766909266953\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9297927701886441\nF1 score: 0.9274917222515475\n"]}],"source":["clf = svm.LinearSVC(max_iter=50000)\n","clf.fit(scaled_train_features, train_labels)\n","labels_pred = clf.predict(scaled_valid_features)\n","print(f'Accuracy: {accuracy_score(valid_labels, labels_pred)}')\n","print(f'F1 score: {f1_score(valid_labels, labels_pred)}')"]},{"metadata":{"id":"Z_5S6UVItjq5","outputId":"9b970b78-8efa-4ca1-db77-86b889c9c01c","trusted":true},"cell_type":"code","source":["column_names = ['id', 'year', 'title', 'authors', 'journal', 'abstract']\n","info = pd.read_csv('node_information.csv', sep=',', names=column_names)\n","info.head()"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     id  year                                              title  \\\n","0  1001  2000              compactification geometry and duality   \n","1  1002  2000  domain walls and massive gauged supergravity p...   \n","2  1003  2000     comment on metric fluctuations in brane worlds   \n","3  1004  2000         moving mirrors and thermodynamic paradoxes   \n","4  1005  2000  bundles of chiral blocks and boundary conditio...   \n","\n","                       authors            journal  \\\n","0            Paul S. Aspinwall                NaN   \n","1  M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n","2     Y.S. Myung, Gungwon Kang                NaN   \n","3               Adam D. Helfer          Phys.Rev.   \n","4      J. Fuchs, C. Schweigert                NaN   \n","\n","                                            abstract  \n","0  these are notes based on lectures given at tas...  \n","1  we point out that massive gauged supergravity ...  \n","2  recently ivanov and volovich hep-th 9912242 cl...  \n","3  quantum fields responding to moving mirrors ha...  \n","4  proceedings of lie iii clausthal july 1999 var...  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>year</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>journal</th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1001</td>\n      <td>2000</td>\n      <td>compactification geometry and duality</td>\n      <td>Paul S. Aspinwall</td>\n      <td>NaN</td>\n      <td>these are notes based on lectures given at tas...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1002</td>\n      <td>2000</td>\n      <td>domain walls and massive gauged supergravity p...</td>\n      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n      <td>Class.Quant.Grav.</td>\n      <td>we point out that massive gauged supergravity ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1003</td>\n      <td>2000</td>\n      <td>comment on metric fluctuations in brane worlds</td>\n      <td>Y.S. Myung, Gungwon Kang</td>\n      <td>NaN</td>\n      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1004</td>\n      <td>2000</td>\n      <td>moving mirrors and thermodynamic paradoxes</td>\n      <td>Adam D. Helfer</td>\n      <td>Phys.Rev.</td>\n      <td>quantum fields responding to moving mirrors ha...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1005</td>\n      <td>2000</td>\n      <td>bundles of chiral blocks and boundary conditio...</td>\n      <td>J. Fuchs, C. Schweigert</td>\n      <td>NaN</td>\n      <td>proceedings of lie iii clausthal july 1999 var...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":21}]},{"metadata":{"id":"7XuO08d_tjq5","trusted":true},"cell_type":"code","source":["with open(\"node_information.csv\", \"r\") as f:\n","    reader = csv.reader(f)\n","    node_info  = list(reader)\n","\n","IDs = [element[0] for element in node_info]"],"execution_count":22,"outputs":[]},{"metadata":{"id":"GpJ246nytjq5","trusted":true},"cell_type":"code","source":["vect = TfidfVectorizer(stop_words=\"english\")\n","abstract_vectorized = vect.fit_transform(info['abstract'])"],"execution_count":23,"outputs":[]},{"metadata":{"id":"mkMQnVAEtjq6","trusted":true},"cell_type":"code","source":["def preprocessing_info(sample, abstract_vectorized):\n","    # number of overlapping words in title\n","    overlap_title = []\n","\n","    # temporal distance between the papers\n","    temp_diff = []\n","\n","    # number of common authors\n","    comm_auth = []\n","\n","    # Cosine sim between abstracts\n","    cosine_sim = []\n","\n","    dense_matrix = abstract_vectorized.todense()\n","\n","    for i in tqdm(range(len(sample))):\n","        source = sample[i][0]\n","        target = sample[i][1]\n","        \n","        index_source = IDs.index(source)\n","        index_target = IDs.index(target)\n","        \n","        source_info = [element for element in node_info if element[0]==source][0]\n","        target_info = [element for element in node_info if element[0]==target][0]\n","        \n","        # convert to lowercase and tokenize\n","        source_title = source_info[2].lower().split(\" \")\n","        # remove stopwords\n","        source_title = [token for token in source_title if token not in stpwds]\n","        source_title = [stemmer.stem(token) for token in source_title]\n","        \n","        target_title = target_info[2].lower().split(\" \")\n","        target_title = [token for token in target_title if token not in stpwds]\n","        target_title = [stemmer.stem(token) for token in target_title]\n","        \n","        source_auth = source_info[3].split(\",\")\n","        target_auth = target_info[3].split(\",\")\n","        \n","        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n","        temp_diff.append(int(source_info[1]) - int(target_info[1]))\n","        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n","\n","        v1 = dense_matrix[index_source,:]\n","        v2 = dense_matrix[index_target,:]\n","\n","        sim = cosine_similarity(v1, v2)\n","        cosine_sim.append(sim[0][0])\n","\n","    return overlap_title, temp_diff, comm_auth, cosine_sim"],"execution_count":24,"outputs":[]},{"metadata":{"id":"DCx1j_DMtjq6","outputId":"f6ce2810-b996-4be0-8787-01ee4b51f61a","trusted":true},"cell_type":"code","source":["overlap_title_train, temp_diff_train, comm_auth_train, cosine_sim_train = preprocessing_info(train_samples, abstract_vectorized)"],"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172180 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c339244d16bf45fe855cc4d26af31730"}},"metadata":{}}]},{"metadata":{"id":"jOT8Qyx6tjq6","outputId":"63eec709-edcb-4a33-94d2-18b00498db8c","trusted":true},"cell_type":"code","source":["overlap_title_valid, temp_diff_valid, comm_auth_valid, cosine_sim_valid = preprocessing_info(valid_samples, abstract_vectorized)"],"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/43044 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41ed6ba60cbe42ca884c6ccceea7ead9"}},"metadata":{}}]},{"metadata":{"id":"CATVmyX-tjq6","trusted":true},"cell_type":"code","source":["def get_training_features(overlap_title, temp_diff, comm_auth, sim):\n","    training_features = np.array([overlap_title, temp_diff, comm_auth, sim]).T\n","    df = pd.DataFrame(training_features, columns=['overl_title', 'temp_diff', 'comm_author', 'sim'])\n","    return training_features, df"],"execution_count":27,"outputs":[]},{"metadata":{"id":"uXJuIDI0tjq6","trusted":true},"cell_type":"code","source":["training_add_feat = get_training_features(overlap_title_train, temp_diff_train, comm_auth_train, cosine_sim_train)[1]\n","valid_add_feat = get_training_features(overlap_title_valid, temp_diff_valid, comm_auth_valid, cosine_sim_valid)[1]"],"execution_count":28,"outputs":[]},{"metadata":{"id":"cbozE9ddtjq6","trusted":true},"cell_type":"code","source":["all_train_feat = pd.concat([feat_train, training_add_feat], axis=1)\n","all_valid_feat = pd.concat([feat_valid, valid_add_feat], axis=1)"],"execution_count":29,"outputs":[]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   source_degree_centrality  target_degree_centrality  pref_attach       aai  \\\n","0                  0.001245                  0.004012        261.0  0.000000   \n","1                  0.001245                  0.002767        180.0  0.248425   \n","2                  0.001245                  0.007748        504.0  0.648466   \n","3                  0.001245                  0.000830         54.0  0.000000   \n","4                  0.005949                  0.003735       1161.0  0.428700   \n","\n","   jacard_coeff   res_all  overl_title  temp_diff  comm_author       sim  \n","0      0.000000  0.000000          1.0        4.0          0.0  0.015978  \n","1      0.035714  0.017857          0.0        2.0          0.0  0.005286  \n","2      0.031746  0.091667          0.0        3.0          0.0  0.023356  \n","3      0.000000  0.000000          1.0        3.0          0.0  0.026137  \n","4      0.029412  0.020333          1.0        1.0          0.0  0.078080  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_degree_centrality</th>\n      <th>target_degree_centrality</th>\n      <th>pref_attach</th>\n      <th>aai</th>\n      <th>jacard_coeff</th>\n      <th>res_all</th>\n      <th>overl_title</th>\n      <th>temp_diff</th>\n      <th>comm_author</th>\n      <th>sim</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001245</td>\n      <td>0.004012</td>\n      <td>261.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.015978</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001245</td>\n      <td>0.002767</td>\n      <td>180.0</td>\n      <td>0.248425</td>\n      <td>0.035714</td>\n      <td>0.017857</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.005286</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001245</td>\n      <td>0.007748</td>\n      <td>504.0</td>\n      <td>0.648466</td>\n      <td>0.031746</td>\n      <td>0.091667</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.023356</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001245</td>\n      <td>0.000830</td>\n      <td>54.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.026137</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.005949</td>\n      <td>0.003735</td>\n      <td>1161.0</td>\n      <td>0.428700</td>\n      <td>0.029412</td>\n      <td>0.020333</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.078080</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":30}],"source":["all_train_feat.head()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9525369389461946\nF1 score: 0.9515704634348702\n0.9882795289093774\n"]}],"source":["# --- Build the model and train it ---\n","# Fill in the blanks\n","#Scale the datas\n","scaler = preprocessing.StandardScaler()\n","scaled_all_train_feat = scaler.fit_transform(all_train_feat)\n","scaled_all_valid_feat = scaler.transform(all_valid_feat)\n","\n","clf = LogisticRegression()\n","clf.fit(scaled_all_train_feat, train_labels)\n","\n","train_preds = clf.predict_proba(scaled_all_train_feat)[:, 1]\n","valid_preds = clf.predict_proba(scaled_all_valid_feat)[:, 1]\n","labels_pred = clf.predict(scaled_all_valid_feat)\n","\n","print(f'Accuracy: {accuracy_score(valid_labels, labels_pred)}')\n","print(f'F1 score: {f1_score(valid_labels, labels_pred)}')\n","\n","# --- Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from predictions ---\n","# Fill in the blanks\n","fpr, tpr, _ = roc_curve(valid_labels, valid_preds)\n","roc_auc = auc(fpr, tpr)\n","print(roc_auc)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9653377938853266\nF1 score: 0.9652877948908845\n"]}],"source":["scaler = preprocessing.StandardScaler()\n","scaled_all_train_feat = scaler.fit_transform(all_train_feat)\n","scaled_all_valid_feat = scaler.transform(all_valid_feat)\n","\n","clf = ensemble.RandomForestClassifier()\n","clf.fit(scaled_all_train_feat, train_labels)\n","\n","train_preds = clf.predict_proba(scaled_all_train_feat)[:, 1]\n","valid_preds = clf.predict_proba(scaled_all_valid_feat)[:, 1]\n","labels_pred = clf.predict(scaled_all_valid_feat)\n","\n","print(f'Accuracy: {accuracy_score(valid_labels, labels_pred)}')\n","print(f'F1 score: {f1_score(valid_labels, labels_pred)}')"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'LinearSVC' object has no attribute 'predict_proba'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-21-22f2b87420e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_all_train_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_all_train_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mvalid_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_all_valid_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mlabels_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_all_valid_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mAttributeError\u001b[0m: 'LinearSVC' object has no attribute 'predict_proba'"]}],"source":["scaler = preprocessing.StandardScaler()\n","scaled_all_train_feat = scaler.fit_transform(all_train_feat)\n","scaled_all_valid_feat = scaler.transform(all_valid_feat)\n","\n","clf = svm.LinearSVC(max_iter=50000)\n","clf.fit(scaled_all_train_feat, train_labels)\n","\n","labels_pred = clf.predict(scaled_all_valid_feat)\n","\n","print(f'Accuracy: {accuracy_score(valid_labels, labels_pred)}')\n","print(f'F1 score: {f1_score(valid_labels, labels_pred)}')"]},{"metadata":{"id":"cU5TYtkztjq7"},"cell_type":"markdown","source":["# Getting the features of the test set"]},{"metadata":{"id":"8qefGIA7tjq7","trusted":true},"cell_type":"code","source":["with open(\"testing_set.txt\", \"r\") as f:\n","    reader = csv.reader(f)\n","    testing_set  = list(reader)\n","\n","testing_set = [element[0].split(\" \") for element in testing_set]"],"execution_count":32,"outputs":[]},{"metadata":{"id":"9c9D3nWAtjq7","outputId":"b4d82f38-fe32-4cc9-c6d5-3bf8f15f2c41","trusted":true},"cell_type":"code","source":["overlap_title_test = []\n","temp_diff_test = []\n","comm_auth_test = []\n","cosine_sim_test = []\n","dense_matrix = abstract_vectorized.todense()\n","   \n","counter = 0\n","for i in tqdm(range(len(testing_set))):\n","\n","    source = testing_set[i][0]\n","    target = testing_set[i][1]\n","    \n","    index_source = IDs.index(source)\n","    index_target = IDs.index(target)\n","    \n","    source_info = [element for element in node_info if element[0]==source][0]\n","    target_info = [element for element in node_info if element[0]==target][0]\n","    \n","    source_title = source_info[2].lower().split(\" \")\n","    source_title = [token for token in source_title if token not in stpwds]\n","    source_title = [stemmer.stem(token) for token in source_title]\n","    \n","    target_title = target_info[2].lower().split(\" \")\n","    target_title = [token for token in target_title if token not in stpwds]\n","    target_title = [stemmer.stem(token) for token in target_title]\n","    \n","    source_auth = source_info[3].split(\",\")\n","    target_auth = target_info[3].split(\",\")\n","    \n","    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n","    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n","    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n","\n","    v1 = dense_matrix[index_source,:]\n","    v2 = dense_matrix[index_target,:]\n","\n","    sim = cosine_similarity(v1, v2)\n","    cosine_sim_test.append(sim[0][0])"],"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32648 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"587479966f63405986d4920b527139ac"}},"metadata":{}}]},{"metadata":{"tags":[],"id":"8OmhI74Atjq7","trusted":true},"cell_type":"code","source":["testing_features = np.array([overlap_title_test,temp_diff_test,comm_auth_test, cosine_sim_test]).T"],"execution_count":34,"outputs":[]},{"metadata":{"id":"Z5yTxPUitjq8","trusted":true},"cell_type":"code","source":["test_feat = pd.DataFrame(testing_features, columns=['overl_title', 'temp_diff', 'comm_author', 'sim'])"],"execution_count":35,"outputs":[]},{"metadata":{"id":"A8kKAycttjq8","trusted":true},"cell_type":"code","source":["test_graph_feat = feature_extractor(G0.to_undirected(), testing_set, deg_centrality)"],"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32648 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffef260c9afd47ccb827aa433d0af57b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["26129\n"]}]},{"metadata":{"id":"qfKW8YKxoALO","trusted":true},"cell_type":"code","source":["test_graph_feat = pd.DataFrame(test_graph_feat, columns=['source_degree_centrality', 'target_degree_centrality', 'pref_attach', 'aai', 'jacard_coeff', 'res_all'])"],"execution_count":37,"outputs":[]},{"metadata":{"id":"hHKP7pIitjq8","trusted":true},"cell_type":"code","source":["all_test_feat = pd.concat([test_graph_feat, test_feat], axis=1)"],"execution_count":38,"outputs":[]},{"metadata":{"id":"ZM8UjM8htjq8","outputId":"634a13e3-0d3c-4b4b-829c-108a8ce7a815","trusted":true},"cell_type":"code","source":["all_test_feat.head()"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   source_degree_centrality  target_degree_centrality  pref_attach       aai  \\\n","0                  0.007056                  0.001522        561.0  0.000000   \n","1                  0.026702                  0.005534       7680.0  5.632175   \n","2                  0.000000                  0.000000          0.0  0.000000   \n","3                  0.005396                  0.006779       1911.0  5.239889   \n","4                  0.000000                  0.000000          0.0  0.000000   \n","\n","   jacard_coeff   res_all  overl_title  temp_diff  comm_author       sim  \n","0      0.000000  0.000000          0.0        0.0          0.0  0.078254  \n","1      0.115385  0.375090          2.0        1.0          0.0  0.174073  \n","2      0.000000  0.000000          1.0        2.0          0.0  0.138734  \n","3      0.313433  0.391564          1.0        0.0          0.0  0.119870  \n","4      0.000000  0.000000          0.0        5.0          0.0  0.304493  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_degree_centrality</th>\n      <th>target_degree_centrality</th>\n      <th>pref_attach</th>\n      <th>aai</th>\n      <th>jacard_coeff</th>\n      <th>res_all</th>\n      <th>overl_title</th>\n      <th>temp_diff</th>\n      <th>comm_author</th>\n      <th>sim</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.007056</td>\n      <td>0.001522</td>\n      <td>561.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.078254</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.026702</td>\n      <td>0.005534</td>\n      <td>7680.0</td>\n      <td>5.632175</td>\n      <td>0.115385</td>\n      <td>0.375090</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.174073</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.138734</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.005396</td>\n      <td>0.006779</td>\n      <td>1911.0</td>\n      <td>5.239889</td>\n      <td>0.313433</td>\n      <td>0.391564</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.119870</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.304493</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":39}]},{"source":["## Save the train, valid and test all feature dataframe"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["#Save the train\n","all_train_feat.to_csv('save/traindf.csv',index=False)\n","pd.Series(train_labels).to_csv('save/trainlabels.csv',index=False)\n","\n","#Save the valid\n","all_valid_feat.to_csv('save/validdf.csv',index=False)\n","pd.Series(valid_labels).to_csv('save/validlabels.csv',index=False)\n","\n","#Save the test\n","all_test_feat.to_csv('save/testdf.csv',index=False)"]},{"source":["## Load the dataframes"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["all_train_feat = pd.read_csv('save/traindf.csv')\n","train_labels = pd.read_csv('save/trainlabels.csv')\n","\n","all_valid_feat = pd.read_csv('save/validdf.csv')\n","valid_labels = pd.read_csv('save/validlabels.csv')\n","\n","all_test_feat = pd.read_csv('save/testdf.csv')\n","\n","\n","#Convert labels from pandas Series to list\n","train_labels = list(train_labels.values.flatten())\n","valid_labels = list(valid_labels.values.flatten())\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["#all_train_feat = pd.concat([all_train_feat, all_valid_feat], ignore_index=True, sort=False)\n","#train_labels = train_labels + valid_labels\n"]},{"source":["## Prediction"],"cell_type":"markdown","metadata":{}},{"metadata":{"id":"ibIvrbN8tjq8","outputId":"c10127b3-6c0f-4b58-c071-2d62f6defff1","trusted":true},"cell_type":"code","source":["#Scale the datas\n","scaler = preprocessing.StandardScaler()\n","scaled_all_train_feat = scaler.fit_transform(all_train_feat)\n","scaled_all_test_feat = scaler.transform(all_test_feat)\n","\n","# initialize basic SVM or LR\n","#classifier = svm.LinearSVC(max_iter=50000)\n","#classifier = LogisticRegression(max_iter=50000)\n","clf = ensemble.RandomForestClassifier()\n","\n","# train\n","classifier.fit(all_train_feat, train_labels)"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(max_iter=50000)"]},"metadata":{},"execution_count":39}]},{"metadata":{"id":"i4MROL0ntjq8","trusted":false},"cell_type":"code","source":["predictions_SVM = list(classifier.predict(scaled_all_test_feat))\n","id = list(range(len(predictions_SVM)))"],"execution_count":40,"outputs":[]},{"source":["## Submission"],"cell_type":"markdown","metadata":{}},{"metadata":{"id":"IDCQJStbtjq9","trusted":false,"tags":[]},"cell_type":"code","source":["submission_df = pd.DataFrame(zip(id,predictions_SVM),columns=['id','category'])\n","submission_df.to_csv('testrf.csv',index=False)"],"execution_count":42,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.8.5-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}