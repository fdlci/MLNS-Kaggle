{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ecbb290622efe6a108e88b8a5dccb3aa582d66a3ed03b3b3ca754b0c02090994"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\inesp\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\inesp\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "source": [
    "# Benchmark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(len(training_set))\n",
    "#print(IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute TFIDF vector of each paper (the abstract)\n",
    "corpus = [element[5] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# each row is a node in the order of node_info\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 5% of training set\n",
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*0.2)))\n",
    "training_set_reduced = [training_set[i] for i in to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "123102\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of overlapping words in title\n",
    "overlap_title = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth = []\n",
    "\n",
    "# Cosine sim between abstracts\n",
    "cosine_sim = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n",
      "16001 training examples processsed\n",
      "17001 training examples processsed\n",
      "18001 training examples processsed\n",
      "19001 training examples processsed\n",
      "20001 training examples processsed\n",
      "21001 training examples processsed\n",
      "22001 training examples processsed\n",
      "23001 training examples processsed\n",
      "24001 training examples processsed\n",
      "25001 training examples processsed\n",
      "26001 training examples processsed\n",
      "27001 training examples processsed\n",
      "28001 training examples processsed\n",
      "29001 training examples processsed\n",
      "30001 training examples processsed\n",
      "31001 training examples processsed\n",
      "32001 training examples processsed\n",
      "33001 training examples processsed\n",
      "34001 training examples processsed\n",
      "35001 training examples processsed\n",
      "36001 training examples processsed\n",
      "37001 training examples processsed\n",
      "38001 training examples processsed\n",
      "39001 training examples processsed\n",
      "40001 training examples processsed\n",
      "41001 training examples processsed\n",
      "42001 training examples processsed\n",
      "43001 training examples processsed\n",
      "44001 training examples processsed\n",
      "45001 training examples processsed\n",
      "46001 training examples processsed\n",
      "47001 training examples processsed\n",
      "48001 training examples processsed\n",
      "49001 training examples processsed\n",
      "50001 training examples processsed\n",
      "51001 training examples processsed\n",
      "52001 training examples processsed\n",
      "53001 training examples processsed\n",
      "54001 training examples processsed\n",
      "55001 training examples processsed\n",
      "56001 training examples processsed\n",
      "57001 training examples processsed\n",
      "58001 training examples processsed\n",
      "59001 training examples processsed\n",
      "60001 training examples processsed\n",
      "61001 training examples processsed\n",
      "62001 training examples processsed\n",
      "63001 training examples processsed\n",
      "64001 training examples processsed\n",
      "65001 training examples processsed\n",
      "66001 training examples processsed\n",
      "67001 training examples processsed\n",
      "68001 training examples processsed\n",
      "69001 training examples processsed\n",
      "70001 training examples processsed\n",
      "71001 training examples processsed\n",
      "72001 training examples processsed\n",
      "73001 training examples processsed\n",
      "74001 training examples processsed\n",
      "75001 training examples processsed\n",
      "76001 training examples processsed\n",
      "77001 training examples processsed\n",
      "78001 training examples processsed\n",
      "79001 training examples processsed\n",
      "80001 training examples processsed\n",
      "81001 training examples processsed\n",
      "82001 training examples processsed\n",
      "83001 training examples processsed\n",
      "84001 training examples processsed\n",
      "85001 training examples processsed\n",
      "86001 training examples processsed\n",
      "87001 training examples processsed\n",
      "88001 training examples processsed\n",
      "89001 training examples processsed\n",
      "90001 training examples processsed\n",
      "91001 training examples processsed\n",
      "92001 training examples processsed\n",
      "93001 training examples processsed\n",
      "94001 training examples processsed\n",
      "95001 training examples processsed\n",
      "96001 training examples processsed\n",
      "97001 training examples processsed\n",
      "98001 training examples processsed\n",
      "99001 training examples processsed\n",
      "100001 training examples processsed\n",
      "101001 training examples processsed\n",
      "102001 training examples processsed\n",
      "103001 training examples processsed\n",
      "104001 training examples processsed\n",
      "105001 training examples processsed\n",
      "106001 training examples processsed\n",
      "107001 training examples processsed\n",
      "108001 training examples processsed\n",
      "109001 training examples processsed\n",
      "110001 training examples processsed\n",
      "111001 training examples processsed\n",
      "112001 training examples processsed\n",
      "113001 training examples processsed\n",
      "114001 training examples processsed\n",
      "115001 training examples processsed\n",
      "116001 training examples processsed\n",
      "117001 training examples processsed\n",
      "118001 training examples processsed\n",
      "119001 training examples processsed\n",
      "120001 training examples processsed\n",
      "121001 training examples processsed\n",
      "122001 training examples processsed\n",
      "123001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "dense_matrix = features_TFIDF.todense()\n",
    "for i in range(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "\n",
    "    v1 = dense_matrix[index_source,:]\n",
    "    v2 = dense_matrix[index_target,:]\n",
    "\n",
    "    sim = cosine_similarity(v1, v2)\n",
    "    cosine_sim.append(sim[0][0])\n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"training examples processsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=10000)"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "training_features = np.array([overlap_title, temp_diff, comm_auth, cosine_sim]).T\n",
    "\n",
    "# scale\n",
    "training_features = preprocessing.scale(training_features)\n",
    "\n",
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC(max_iter=10000)\n",
    "\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 testing examples processsed\n",
      "1001 testing examples processsed\n",
      "2001 testing examples processsed\n",
      "3001 testing examples processsed\n",
      "4001 testing examples processsed\n",
      "5001 testing examples processsed\n",
      "6001 testing examples processsed\n",
      "7001 testing examples processsed\n",
      "8001 testing examples processsed\n",
      "9001 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "11001 testing examples processsed\n",
      "12001 testing examples processsed\n",
      "13001 testing examples processsed\n",
      "14001 testing examples processsed\n",
      "15001 testing examples processsed\n",
      "16001 testing examples processsed\n",
      "17001 testing examples processsed\n",
      "18001 testing examples processsed\n",
      "19001 testing examples processsed\n",
      "20001 testing examples processsed\n",
      "21001 testing examples processsed\n",
      "22001 testing examples processsed\n",
      "23001 testing examples processsed\n",
      "24001 testing examples processsed\n",
      "25001 testing examples processsed\n",
      "26001 testing examples processsed\n",
      "27001 testing examples processsed\n",
      "28001 testing examples processsed\n",
      "29001 testing examples processsed\n",
      "30001 testing examples processsed\n",
      "31001 testing examples processsed\n",
      "32001 testing examples processsed\n"
     ]
    }
   ],
   "source": [
    "overlap_title_test = []\n",
    "temp_diff_test = []\n",
    "comm_auth_test = []\n",
    "cosine_sim_test = []\n",
    "   \n",
    "counter = 0\n",
    "for i in range(len(testing_set)):\n",
    "\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "\n",
    "    v1 = dense_matrix[index_source,:]\n",
    "    v2 = dense_matrix[index_target,:]\n",
    "\n",
    "    sim = cosine_similarity(v1, v2)\n",
    "    cosine_sim_test.append(sim[0][0])\n",
    "   \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "        print(counter, \"testing examples processsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32648, 4)\n"
     ]
    }
   ],
   "source": [
    "testing_features = np.array([overlap_title_test,temp_diff_test,comm_auth_test, cosine_sim_test]).T\n",
    "\n",
    "print(testing_features.shape)\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)\n",
    "\n",
    "# issue predictions\n",
    "predictions_SVM = list(classifier.predict(testing_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<zip object at 0x0000020920306DC0>\n"
     ]
    }
   ],
   "source": [
    "predictions_SVM = zip(range(len(testing_set)), predictions_SVM)\n",
    "print(predictions_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"improved_predictions_20%.csv\",\"w\", newline='') as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    for row in predictions_SVM:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}